spark.appName = spark-jdbc
spark.pollingInterval = 2

#source.inputType = twitter
#source.inputType = socket
#source.inputType = folder
source.inputType = kafka
source.inputFolder = C:/Data/test-streaming/input
source.inputSocketHost = ws.blockchain.info
source.inputSocketPort = 80
#source.inputSocketPort = 19998

source.jdbc.url=jdbc:postgresql://ubuntuserver:5432/postgres
source.jdbc.user=postgres
source.jdbc.password=Patatoide
source.jdbc.table=public.apple_tweets

#target.jdbc.url=jdbc:mysql://ubuntuserver:3306/test?transformedBitIsBoolean=false&tinyInt1isBit=false&useUnicode=true&connectionCollation=utf8_general_ci
#target.jdbc.user=test
#target.jdbc.password=test
#target.jdbc.table=test.tab_target

target.jdbc.url=jdbc:h2:tcp://ubuntuserver:19092/test
target.jdbc.user=
target.jdbc.password=
target.jdbc.table=tab_target


#hadoopTargetPrefix = /data/streaming

# Type of DB target direct (scala jdbc) vs dataframe
#spark.target = direct
spark.target = dataframe
#spark.target = console

# DB-Configuration for H2
#db.driver = org.h2.Driver
#db.url = jdbc:h2:file:C:/Data/test-streaming/output/test

# DB-Configuration for MySQL
db.driver = com.mysql.jdbc.Driver
db.url = jdbc:mysql://ubuntuserver/test
db.username = test
db.password = test

# DB-Configuration for MySQL
#db.driver = com.mysql.jdbc.Driver
#db.url = jdbc:mysql://ubuntu/test
#db.username = test
#db.password = test

# DB-Configuration for HANA
#db.driver = com.sap.db.jdbc.Driver
#db.url = jdbc:sap://hana:30115
#db.username = test
#db.password = test

db.tableName = streams
db.tableSchema = test
#db.tableName = streams
#db.tableSchema = streams
#db.whereClause = severity = 'WARNING'

db.createSQL = CREATE TABLE test_stream(test_col0 VARCHAR(256),test_col1 VARCHAR(256))
db.insertSQL = INSERT INTO test_stream(test_col0,test_col1) VALUES (?, ?)

kafka.bootstrap.servers = ubuntuserver:9092
kafka.group.id = test-group
kafka.auto.offset.reset = earliest
kafka.enable.auto.commit =false
